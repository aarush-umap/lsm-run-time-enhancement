{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "from skimage import img_as_uint\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tifffile\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "#import torch\n",
    "#torch.cuda.set_per_process_memory_fraction(0.5, 0)\n",
    "\n",
    "from denoiser import Denoiser\n",
    "from lsm_utils import normalize_16bit_images, compute_norm_range\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data \n",
    "- SHG samples\n",
    "- load model config data from yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data\\*.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n"
     ]
    }
   ],
   "source": [
    "def sandbox_compute_norm_range(fname, percentiles=(0, 100), sample_r=0.1):\n",
    "    max_val = []\n",
    "    min_val = []\n",
    "    fail_names = []\n",
    "    try:\n",
    "        img = img_as_uint(io.imread(fname))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(fname)\n",
    "        fail_names.append(fname)\n",
    "    max_val.append(np.percentile(img[:512,:512], percentiles[1]))\n",
    "    min_val.append(np.percentile(img[:512,:512], percentiles[0]))\n",
    "    max_val: float | np.ndarray = np.percentile(np.array(max_val), 98)\n",
    "    min_val: float | np.ndarray = np.percentile(np.array(min_val), 2)\n",
    "    \n",
    "    return min_val, max_val, fail_names\n",
    "\n",
    "# vmin, vmax, fail_names = sandbox_compute_norm_range('sample_data/1B_C1.tif', percentiles=(1, 99.5), sample_r=0.05)\n",
    "vmin, vmax, fail_names = compute_norm_range('sample_data', ext='tif', percentiles=(1, 99.5), sample_r=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(open(\"model_config.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "config['dataset'] = 'sample_data'\n",
    "config['norm-range'] = [int(vmin), int(vmax)]\n",
    "config['threads'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model instance\n",
    "* Create instance of denoiser with new config data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\lociuser/.cache\\torch\\hub\\mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    }
   ],
   "source": [
    "# Set background screening to false to allow PB522-14-MAX-Fused.tif to be accepted\n",
    "denoiser = Denoiser(config, screen_bg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoiser only works on grayscale images\n",
    "denoiser.denoise(sampling=True, sample_rate=.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data\\*.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.50it/s]\n",
      "Using cache found in C:\\Users\\lociuser/.cache\\torch\\hub\\mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    }
   ],
   "source": [
    "vmin, vmax, fail_names = compute_norm_range('sample_data', ext='tif', percentiles=(1, 99.5), sample_r=1)\n",
    "config = yaml.load(open(\"model_config.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "config['dataset'] = 'sample_data'\n",
    "config['norm-range'] = [int(vmin), int(vmax)]\n",
    "config['threads'] = 0\n",
    "denoiser = Denoiser(config, screen_bg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lociuser\\Documents\\GitHub\\lsm-run-time-enhancement\\denoise_sandbox.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lociuser/Documents/GitHub/lsm-run-time-enhancement/denoise_sandbox.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m img_input \u001b[39m=\u001b[39m exposure\u001b[39m.\u001b[39mrescale_intensity(img_arr, in_range\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m65535\u001b[39m), out_range\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lociuser/Documents/GitHub/lsm-run-time-enhancement/denoise_sandbox.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m img_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(img_input)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lociuser/Documents/GitHub/lsm-run-time-enhancement/denoise_sandbox.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m img_hyper_tensor \u001b[39m=\u001b[39m img_tensor\u001b[39m.\u001b[39mexpand([\u001b[39m50\u001b[39m, \u001b[39m1\u001b[39m, img_tensor\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], img_tensor\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]])\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lociuser/Documents/GitHub/lsm-run-time-enhancement/denoise_sandbox.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m out_tensor \u001b[39m=\u001b[39m img_tensor \u001b[39m*\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lociuser/Documents/GitHub/lsm-run-time-enhancement/denoise_sandbox.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(iterations):\n",
      "\u001b[1;32mc:\\Users\\lociuser\\Documents\\GitHub\\lsm-run-time-enhancement\\denoise_sandbox.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lociuser/Documents/GitHub/lsm-run-time-enhancement/denoise_sandbox.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m img_input \u001b[39m=\u001b[39m exposure\u001b[39m.\u001b[39mrescale_intensity(img_arr, in_range\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m65535\u001b[39m), out_range\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lociuser/Documents/GitHub/lsm-run-time-enhancement/denoise_sandbox.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m img_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(img_input)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lociuser/Documents/GitHub/lsm-run-time-enhancement/denoise_sandbox.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m img_hyper_tensor \u001b[39m=\u001b[39m img_tensor\u001b[39m.\u001b[39mexpand([\u001b[39m50\u001b[39m, \u001b[39m1\u001b[39m, img_tensor\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], img_tensor\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]])\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lociuser/Documents/GitHub/lsm-run-time-enhancement/denoise_sandbox.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m out_tensor \u001b[39m=\u001b[39m img_tensor \u001b[39m*\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lociuser/Documents/GitHub/lsm-run-time-enhancement/denoise_sandbox.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(iterations):\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\lociuser\\mambaforge\\envs\\lsm-run-time-enhancement\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lociuser\\mambaforge\\envs\\lsm-run-time-enhancement\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[0;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from skimage import exposure\n",
    "import torch.nn.functional as F \n",
    "\n",
    "p = config['blindspot-rate']\n",
    "pass_times = int(1/p * config['average-factor'])\n",
    "iterations = int(np.ceil(pass_times/50))\n",
    "model = denoiser.backbone\n",
    "device = next(model.parameters()).device\n",
    "fname = \"sample_data/PB522-14-MAX_Fused.tif\"\n",
    "img_arr = img_as_uint(io.imread(fname))[:512, :512]\n",
    "img_arr = exposure.rescale_intensity(img_arr, in_range=(config['norm-range'][0], config['norm-range'][1]), out_range=(0, 65535)).astype(int)\n",
    "img_input = exposure.rescale_intensity(img_arr, in_range=(0, 65535), out_range=(0, 1))\n",
    "img_tensor = torch.from_numpy(img_input)\n",
    "img_hyper_tensor = img_tensor.expand([50, 1, img_tensor.shape[0], img_tensor.shape[1]]).float().to(device)\n",
    "out_tensor = img_tensor * 0\n",
    "for i in range(iterations):\n",
    "    drop_mask = F.dropout(torch.ones(img_hyper_tensor.shape, requires_grad=False).to(device), p=p, inplace=True)*(1-p) # p percent zero, keep\n",
    "    pad_mask = (1-drop_mask) * torch.ones(img_hyper_tensor.shape, device=device, dtype=torch.float32) * torch.mean(img_hyper_tensor, (2, 3), keepdim=True).expand_as(img_hyper_tensor)\n",
    "    spotted = torch.mul(img_hyper_tensor, drop_mask) + pad_mask\n",
    "    prediction = model(spotted)\n",
    "    prediction = torch.mul(prediction, 1-drop_mask)/p\n",
    "    out_tensor += torch.mean(prediction, 0).squeeze().cpu()/iterations\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    out_arr = img_as_uint(np.clip(out_tensor.numpy().squeeze(), 0, 1))\n",
    "    img_name = os.path.basename(fname)\n",
    "    io.imsave(os.path.join(\"output-self/sample_data/clean/\", img_name), out_arr)\n",
    "    io.imsave(os.path.join(\"output-self/sample_data/noisy/\", img_name), img_as_uint(img_arr))\n",
    "print(f'Processed', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsm-run-time-enhancement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
